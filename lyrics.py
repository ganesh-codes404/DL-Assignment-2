# -*- coding: utf-8 -*-
"""Lyrics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tdQ72kegxUVTS5BjlYDbN38qOwMCDUfM
"""

!pip install -q transformers datasets accelerate

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
poetry_dir = '/content/drive/My Drive/archive-2'
texts = []
for filename in os.listdir(poetry_dir):
    if filename.endswith('.txt'):
        with open(os.path.join(poetry_dir, filename), 'r', encoding='utf-8') as f:
            text = f.read().strip()
            if len(text) > 50:
                texts.append(text)
df = pd.DataFrame(texts, columns=["text"])
print(f"Loaded viable poems", len(df))
df.head()

from transformers import GPT2Tokenizer
from datasets import Dataset

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

dataset = Dataset.from_pandas(df)

def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=256)
tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=["text"])

from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import torch

model = GPT2LMHeadModel.from_pretrained("gpt2")

# Training configuration
training_args = TrainingArguments(
    output_dir="./gpt2-lyrics",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=500,
    logging_steps=50,
    fp16=torch.cuda.is_available(),
    report_to="none"
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Trainer initialization
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator
)
trainer.train()

prompt = "When the sun rises and stars begin to fade"
inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(model.device)
output = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_length=100,
    do_sample=True,
    top_k=50,
    top_p=0.9,
    temperature=1.0,
    repetition_penalty=1.2,
    num_return_sequences=1,
    pad_token_id=tokenizer.pad_token_id
)

print("Generated Lyrics:\n")
print(tokenizer.decode(output[0], skip_special_tokens=True))

